
# word tokenization

from nltk import word_tokenize
sentence = "Natural language processing is one of the more challenging fields in AI."
words = word_tokenize(sentence)
print(words)

